{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import load_data, accuracy, normalize, load_polblogs_data\n",
    "from models import GCN\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "import os.path as op\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "class args:\n",
    "    cuda = True\n",
    "    fastmode = False\n",
    "#     seed = 123\n",
    "    seed = 20\n",
    "    epochs = 200\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    hidden = 16\n",
    "    dropout = 0.5\n",
    "#     pert_num = 20\n",
    "    L1 = 0.5\n",
    "    L2 = 0.1\n",
    "    dataset = \"cora\"\n",
    "    radius = 4\n",
    "\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"polblogs\":\n",
    "    tmp_adj, features, labels, idx_train, idx_test = load_polblogs_data()\n",
    "else:\n",
    "    _, features, labels, idx_train, idx_val, idx_test, tmp_adj  = load_data(args.dataset)\n",
    "\n",
    "num_classes = labels.max().item() + 1\n",
    "# tmp_adj = tmp_adj.toarray()\n",
    "\n",
    "adj = tmp_adj\n",
    "adj = np.eye(tmp_adj.shape[0]) + adj\n",
    "adj, _ = normalize(adj)\n",
    "adj = torch.from_numpy(adj.astype(np.float32))\n",
    "\n",
    "\n",
    "# print (sum(features))\n",
    "# print (labels.shape)\n",
    "# print (idx_train.shape)\n",
    "# print (idx_val.shape)\n",
    "# print (idx_test)\n",
    "# s = adj.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=num_classes,\n",
    "            dropout=args.dropout\n",
    "           )\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    if args.dataset != \"polblogs\":\n",
    "        idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = Variable(adj, requires_grad=True)\n",
    "    output = model(features, x)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if args.dataset != \"polblogs\": \n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "    else:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(adj_m):\n",
    "    model.eval()\n",
    "    output = model(features, adj_m)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# torch.save(model, './cora_gcn.pth')\n",
    "# torch.save(model.state_dict(), 'cora_gcn.pkl')\n",
    "\n",
    "# Testing\n",
    "ori_output = test(adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad(pert_adj, idx, classes):\n",
    "    x = Variable(pert_adj, requires_grad=True)\n",
    "    output = model(features, x)\n",
    "    grad = []\n",
    "#     for i in range(classes):\n",
    "    for i in classes:\n",
    "        cls = torch.LongTensor(np.array(i).reshape(1)).cuda()\n",
    "        loss = F.nll_loss(output[idx:idx+1], cls) \n",
    "        loss.backward(retain_graph=True)\n",
    "        grad.append(x.grad[idx].cpu().numpy())\n",
    "#     print ('grad', grad)\n",
    "    return np.array(grad)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_perturb(input_adj, idx, perturb):\n",
    "    # (1-x)A + x(1-A)\n",
    "#     input_adj = input_adj.toarray()\n",
    "    x = np.zeros((input_adj.shape[0], input_adj.shape[1]))\n",
    "    x[idx] = perturb  \n",
    "    x[:,idx] = perturb\n",
    "#     print ('x', x[idx])\n",
    "\n",
    "    \n",
    "#     x += np.transpose(x) #change the idx'th row and column\n",
    "    x1 = np.ones((input_adj.shape[0], input_adj.shape[1])) - x\n",
    "#     print ('x1', x1[idx])\n",
    "    adj2 = np.ones((input_adj.shape[0], input_adj.shape[1])) - input_adj\n",
    "#     print ('adj2', adj2[idx])\n",
    "\n",
    "    for i in range(input_adj.shape[0]):   \n",
    "        adj2[i][i] = 0\n",
    "\n",
    "    perturbed_adj = np.multiply(x1, input_adj) + np.multiply(x, adj2)\n",
    "    return perturbed_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_lp(v, xi=args.radius, p=2):\n",
    "# def proj_lp(v, xi=8, p=2):\n",
    "\n",
    "    # Project on the lp ball centered at 0 and of radius xi\n",
    "\n",
    "    # SUPPORTS only p = 2 and p = Inf for now\n",
    "#     print ('the distance of v', np.linalg.norm(v.flatten(1)))\n",
    "    \n",
    "    if p == 2:\n",
    "        v = v * min(1, xi/np.linalg.norm(v.flatten(1)))\n",
    "        # v = v / np.linalg.norm(v.flatten(1)) * xi\n",
    "    elif p == np.inf:\n",
    "        v = np.sign(v) * np.minimum(abs(v), xi)\n",
    "    else:\n",
    "        v = v\n",
    "        #################\n",
    "    v = np.clip(v, 0, 1)\n",
    "        ########################\n",
    "#     v = np.where(v<0.1, 0, v)\n",
    "    #to reduce the number of nonzero elements which means \n",
    "    #the times of perturbation, also prevents saddle point\n",
    "\n",
    "#     v = np.where(v>0.5, 1, 0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pert(pert_m):\n",
    "    tmp_pert_m = np.absolute(pert_m)\n",
    "    sort_idx = tmp_pert_m.argsort()[::-1]\n",
    "    sel_idx = np.zeros(pert_m.shape[0])\n",
    "    sel_idx[sort_idx[:args.pert_num]] = 1\n",
    "    return sel_idx     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_v(adj, pert_m, deg, idx):\n",
    "\n",
    "    a = np.multiply(pert_m, deg)\n",
    "    inv_m = np.ones(adj.shape[0]) - np.multiply(adj[idx], 2) \n",
    "    inv_m = np.power(inv_m, -1)\n",
    "    res = np.multiply(a, inv_m)  \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_add_perturb(ori_adj, idx, pert, d):\n",
    "    a = ori_adj[idx] + pert\n",
    "    inv_d = 1 + sum(pert)\n",
    "    p_d = d * inv_d\n",
    "    inv_d = 1.0/inv_d\n",
    "    ## filter the perturbed matrix so that >= 0 \n",
    "#     a = np.where(a<0, 0, a)\n",
    "    ori_adj[idx] = np.multiply(a, inv_d)\n",
    "    \n",
    "    return ori_adj, p_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfool(innormal_adj, ori_adj, idx, num_classes, degree, overshoot=0.02, max_iter=30):\n",
    "    #innormal_adj: the perturbed adjacency matrix not normalized\n",
    "    #ori_adj: the normalized perturbed adjacency matrix \n",
    "    model.eval()\n",
    "    pred = model(features, ori_adj)[idx]\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    \n",
    "    I = pred.argsort()[::-1]\n",
    "    I = I[0:num_classes]\n",
    "    label = I[0]    \n",
    "    f_i = np.array(pred).flatten()\n",
    "    k_i = int(np.argmax(f_i))  \n",
    "    w = np.zeros(ori_adj.shape[0])\n",
    "    r_tot = np.zeros(ori_adj.size(0))\n",
    "    \n",
    "#     pert_adj = ori_adj\n",
    "    pert_adj = ori_adj.detach().cpu().numpy()\n",
    "    pert_adj_tensor = ori_adj\n",
    "    degree_idx = degree\n",
    "    loop_i = 0\n",
    "#     print ('the correct class', label)\n",
    "    while k_i == label and loop_i < max_iter:\n",
    "        pert = np.inf\n",
    "#         gradients = calculate_grad(pert_adj_tensor, idx, num_classes)\n",
    "        gradients = calculate_grad(pert_adj_tensor, idx, I)\n",
    "        for i in range(1, num_classes):\n",
    "            # set new w_k and new f_k\n",
    "            w_k = gradients[i, :] - gradients[0, :]\n",
    "            f_k = f_i[I[i]] - f_i[I[0]]\n",
    "            pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())\n",
    "#             print ('num_classes', num_classes)\n",
    "#             print ('pert_k', pert_k)\n",
    "#             print ('w_k', w_k)\n",
    "\n",
    "            # determine which w_k to use\n",
    "            if pert_k < pert:\n",
    "                pert = pert_k\n",
    "                w = w_k\n",
    "        #this is for the polblogs\n",
    "#         if sum(w) == 0:\n",
    "#             break\n",
    "        # compute r_i and r_tot \n",
    "        r_i =  pert * w / np.linalg.norm(w)\n",
    "#         r_i = convert_to_v(innormal_adj, r_i, idx) #x_change = A'_change / (1-2A)\n",
    "        r_tot = r_tot + r_i\n",
    "        pert_adj, _ = normalize_add_perturb(pert_adj, idx, (1+overshoot)*r_tot, degree_idx)\n",
    "            ##################\n",
    "        pert_adj = np.clip(pert_adj, 0, 1)\n",
    "            ########################\n",
    "#         pert_adj, _ = normalize(pert_adj + np.eye(ori_adj.shape[0]))\n",
    "        loop_i += 1\n",
    "        \n",
    "        # compute new label\n",
    "        pert_adj_tensor = torch.from_numpy(pert_adj.astype(np.float32))\n",
    "        pert_adj_tensor = pert_adj_tensor.cuda()\n",
    "        f_i = np.array(model(features, pert_adj_tensor)[idx].detach().cpu().numpy()).flatten()\n",
    "        k_i = int(np.argmax(f_i))\n",
    "#         print ('degree', degree[idx])\n",
    "#         print ('original conn', ori_adj[idx])\n",
    "#         print ('r_tot', r_tot)\n",
    "#         print ('perturbed conn row', pert_adj[idx])\n",
    "#         print ('output', f_i)\n",
    "#         print ('the predict class', k_i)\n",
    "        if k_i != label:\n",
    "            print ('attack succeeds')\n",
    "    \n",
    "    r_tot = (1+overshoot)*r_tot\n",
    "#     print ('the r_tot', r_tot)\n",
    "    r_tot = convert_to_v(innormal_adj, r_tot, degree_idx, idx)\n",
    "\n",
    "    return r_tot, loop_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def universal_attack(attack_epoch, max_epoch):\n",
    "    model.eval()\n",
    "    delta = 0.1\n",
    "    fooling_rate = 0.0\n",
    "    overshoot = 0.02\n",
    "    max_iter_df = 10\n",
    "\n",
    "    v = np.zeros(tmp_adj.shape[0]).astype(np.float32)\n",
    "    # stdv = 1./math.sqrt(tmp_adj.shape[0])\n",
    "    # v = np.random.uniform(-stdv, stdv, tmp_adj.shape[0])\n",
    "\n",
    "    cur_foolingrate = 0.0\n",
    "    epoch = 0\n",
    "    \n",
    "    early_stop = 0\n",
    "    results = []\n",
    "    folder_path = op.join(\"./\", \"perturbation_results\")\n",
    "    if not op.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    \n",
    "    while fooling_rate < 1 - delta and epoch < max_epoch:\n",
    "        epoch += 1\n",
    "        train_idx = idx_train.cpu().numpy()\n",
    "        np.random.shuffle(train_idx)\n",
    "        \n",
    "        ###############################################\n",
    "        print ('deepfooling...')\n",
    "        attack_time = time.time()\n",
    "        for k in train_idx:\n",
    "            print ('deepfool node',k)\n",
    "            #add v to see if the attack succeeds\n",
    "            innormal_x_p = add_perturb(tmp_adj, k, v)\n",
    "\n",
    "            ##################whether to use filtering\n",
    "    #         innormal_x_p = np.where(innormal_x_p<0.5, 0, 1)\n",
    "\n",
    "            x_p, degree_p = normalize(innormal_x_p + np.eye(tmp_adj.shape[0])) #A' = A + I\n",
    "            x_p = torch.from_numpy(x_p.astype(np.float32))\n",
    "            x_p = x_p.cuda()\n",
    "\n",
    "            output = model(features, x_p)\n",
    "\n",
    "            if int(torch.argmax(output[k])) == int(torch.argmax(ori_output[k])):\n",
    "                dr, iter = deepfool(innormal_x_p, x_p, k, num_classes, degree_p[k])\n",
    "\n",
    "                if iter < max_iter_df-1:\n",
    "                    v = v + dr\n",
    "\n",
    "                    # Project on l_p ball\n",
    "                    v = proj_lp(v)\n",
    "    #                 print ('L1 norm ov v', torch.norm(v, p=1))\n",
    "    #                 print ('L2 norm ov v', torch.norm(v, p=2))\n",
    "                else:\n",
    "                    print ('cant attack this node')\n",
    "\n",
    "    #         print ('the prediction of k node', int(torch.argmax(output[k])))\n",
    "    #         print ('the true label', int(labels[k]))\n",
    "            else:\n",
    "                print ('attack succeeds')\n",
    "        print ('the deepfooling time cost is', time.time()-attack_time)\n",
    "\n",
    "        ###################################################\n",
    "    #     v = np.random.rand(tmp_adj.shape[0])\n",
    "\n",
    "\n",
    "        print ('the perturbation matrix is', v)\n",
    "        print ('calculate the whole attack success rate over the train nodes')\n",
    "        res = []\n",
    "        v = np.where(v>0.5, 1, 0)\n",
    "        for k in train_idx:\n",
    "            print ('test node', k)\n",
    "            innormal_x_p = add_perturb(tmp_adj, k, v)            \n",
    "#             innormal_x_p = np.where(innormal_x_p<0.5, 0, 1)\n",
    "            \n",
    "            x_p, degree_p = normalize(innormal_x_p + np.eye(tmp_adj.shape[0]))\n",
    "            x_p = torch.from_numpy(x_p.astype(np.float32))\n",
    "            x_p = x_p.cuda()\n",
    "            output = model(features, x_p)\n",
    "            if int(torch.argmax(output[k])) == int(torch.argmax(ori_output[k])):\n",
    "                res.append(0)\n",
    "            else:\n",
    "                res.append(1)\n",
    "        fooling_rate = float(sum(res)/len(res))\n",
    "        print ('the current train fooling rates are', fooling_rate)\n",
    "\n",
    "        if fooling_rate > cur_foolingrate:\n",
    "            cur_foolingrate = fooling_rate\n",
    "            file_path = op.join(folder_path, '{1}_xi{2}_epoch100/perturbation_{1}_{0}.txt'.format(attack_epoch, args.dataset, args.radius))\n",
    "            with open(file_path) as f:\n",
    "                for i in v:\n",
    "                    f.write(str(i) + '\\n')\n",
    "        results.append(fooling_rate)\n",
    "        if epoch > 3:\n",
    "            if fooling_rate == results[-2]:\n",
    "                early_stop += 1\n",
    "            else:\n",
    "                early_stop = 0\n",
    "        if early_stop == 15:\n",
    "            break\n",
    "        \n",
    "    return cur_foolingrate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_foolrate = []\n",
    "for i in range(0,10):\n",
    "    fool_rate = universal_attack(i, 100)\n",
    "    train_foolrate.append(fool_rate)\n",
    "print ('the final train fool rate', train_foolrate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
