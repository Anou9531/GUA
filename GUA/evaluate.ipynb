{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import load_data, accuracy, normalize, load_polblogs_data\n",
    "from models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    cuda = True\n",
    "    fastmode = False\n",
    "    seed = 20\n",
    "#     seed = 123\n",
    "    epochs = 200\n",
    "    lr = 0.01\n",
    "    weight_decay = 5e-4\n",
    "    hidden = 16\n",
    "    dropout = 0.5\n",
    "    pert_num = 20\n",
    "    L1 = 0.01\n",
    "    evaluate_mode = \"universal\"\n",
    "    dataset = \"cora\"\n",
    "    radius = 4\n",
    "\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"polblogs\":\n",
    "    tmp_adj, features, labels, idx_train, idx_test = load_polblogs_data()\n",
    "    print (sum(sum(tmp_adj)))\n",
    "    print (tmp_adj.shape)\n",
    "else:\n",
    "    _, features, labels, idx_train, idx_val, idx_test, tmp_adj  = load_data(args.dataset)\n",
    "\n",
    "num_classes = labels.max().item() + 1\n",
    "# tmp_adj = tmp_adj.toarray()\n",
    "\n",
    "adj = tmp_adj\n",
    "adj = np.eye(tmp_adj.shape[0]) + adj\n",
    "adj, _ = normalize(adj)\n",
    "adj = torch.from_numpy(adj.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=num_classes,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    if args.dataset != \"polblogs\":\n",
    "        idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = Variable(adj, requires_grad=True)\n",
    "    output = model(features, x)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "#     print ('output', output.size())\n",
    "#     print ('labels', labels.size())\n",
    "    loss_train.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    if args.dataset != \"polblogs\": \n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "    else:\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(adj_m):\n",
    "    model.eval()\n",
    "    output = model(features, adj_m)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9839 acc_train: 0.1429 loss_val: 2.0281 acc_val: 0.0540 time: 0.2897s\n",
      "Epoch: 0002 loss_train: 1.9739 acc_train: 0.1357 loss_val: 2.0129 acc_val: 0.0600 time: 0.0042s\n",
      "Epoch: 0003 loss_train: 1.9671 acc_train: 0.1429 loss_val: 2.0013 acc_val: 0.0600 time: 0.0036s\n",
      "Epoch: 0004 loss_train: 1.9605 acc_train: 0.1571 loss_val: 1.9992 acc_val: 0.0580 time: 0.0034s\n",
      "Epoch: 0005 loss_train: 1.9507 acc_train: 0.1500 loss_val: 1.9846 acc_val: 0.0600 time: 0.0027s\n",
      "Epoch: 0006 loss_train: 1.9440 acc_train: 0.1429 loss_val: 1.9786 acc_val: 0.0540 time: 0.0030s\n",
      "Epoch: 0007 loss_train: 1.9418 acc_train: 0.1571 loss_val: 1.9712 acc_val: 0.0640 time: 0.0033s\n",
      "Epoch: 0008 loss_train: 1.9395 acc_train: 0.1357 loss_val: 1.9656 acc_val: 0.0680 time: 0.0032s\n",
      "Epoch: 0009 loss_train: 1.9249 acc_train: 0.1500 loss_val: 1.9563 acc_val: 0.0720 time: 0.0036s\n",
      "Epoch: 0010 loss_train: 1.9293 acc_train: 0.1714 loss_val: 1.9516 acc_val: 0.0640 time: 0.0035s\n",
      "Epoch: 0011 loss_train: 1.9213 acc_train: 0.1857 loss_val: 1.9413 acc_val: 0.0920 time: 0.0037s\n",
      "Epoch: 0012 loss_train: 1.9121 acc_train: 0.1929 loss_val: 1.9384 acc_val: 0.0820 time: 0.0038s\n",
      "Epoch: 0013 loss_train: 1.9151 acc_train: 0.2214 loss_val: 1.9310 acc_val: 0.1240 time: 0.0030s\n",
      "Epoch: 0014 loss_train: 1.9022 acc_train: 0.3000 loss_val: 1.9191 acc_val: 0.1560 time: 0.0032s\n",
      "Epoch: 0015 loss_train: 1.9017 acc_train: 0.3214 loss_val: 1.9087 acc_val: 0.2140 time: 0.0032s\n",
      "Epoch: 0016 loss_train: 1.8894 acc_train: 0.4000 loss_val: 1.9079 acc_val: 0.2120 time: 0.0038s\n",
      "Epoch: 0017 loss_train: 1.8948 acc_train: 0.3500 loss_val: 1.9090 acc_val: 0.2440 time: 0.0031s\n",
      "Epoch: 0018 loss_train: 1.8853 acc_train: 0.3643 loss_val: 1.9007 acc_val: 0.3060 time: 0.0034s\n",
      "Epoch: 0019 loss_train: 1.8890 acc_train: 0.3214 loss_val: 1.8961 acc_val: 0.2900 time: 0.0032s\n",
      "Epoch: 0020 loss_train: 1.8637 acc_train: 0.4000 loss_val: 1.8898 acc_val: 0.2960 time: 0.0030s\n",
      "Epoch: 0021 loss_train: 1.8623 acc_train: 0.3500 loss_val: 1.8819 acc_val: 0.3240 time: 0.0038s\n",
      "Epoch: 0022 loss_train: 1.8626 acc_train: 0.4786 loss_val: 1.8819 acc_val: 0.3420 time: 0.0032s\n",
      "Epoch: 0023 loss_train: 1.8475 acc_train: 0.4643 loss_val: 1.8738 acc_val: 0.3600 time: 0.0035s\n",
      "Epoch: 0024 loss_train: 1.8360 acc_train: 0.5643 loss_val: 1.8774 acc_val: 0.3620 time: 0.0039s\n",
      "Epoch: 0025 loss_train: 1.8221 acc_train: 0.5857 loss_val: 1.8674 acc_val: 0.3880 time: 0.0037s\n",
      "Epoch: 0026 loss_train: 1.8205 acc_train: 0.5571 loss_val: 1.8665 acc_val: 0.3840 time: 0.0034s\n",
      "Epoch: 0027 loss_train: 1.8043 acc_train: 0.6000 loss_val: 1.8583 acc_val: 0.3940 time: 0.0032s\n",
      "Epoch: 0028 loss_train: 1.8000 acc_train: 0.6786 loss_val: 1.8574 acc_val: 0.4440 time: 0.0032s\n",
      "Epoch: 0029 loss_train: 1.8042 acc_train: 0.6286 loss_val: 1.8521 acc_val: 0.4700 time: 0.0037s\n",
      "Epoch: 0030 loss_train: 1.7795 acc_train: 0.6786 loss_val: 1.8431 acc_val: 0.4440 time: 0.0037s\n",
      "Epoch: 0031 loss_train: 1.7789 acc_train: 0.7000 loss_val: 1.8447 acc_val: 0.4600 time: 0.0034s\n",
      "Epoch: 0032 loss_train: 1.7571 acc_train: 0.7071 loss_val: 1.8376 acc_val: 0.4760 time: 0.0034s\n",
      "Epoch: 0033 loss_train: 1.7314 acc_train: 0.7857 loss_val: 1.8229 acc_val: 0.4920 time: 0.0037s\n",
      "Epoch: 0034 loss_train: 1.7268 acc_train: 0.7429 loss_val: 1.8233 acc_val: 0.5000 time: 0.0033s\n",
      "Epoch: 0035 loss_train: 1.7262 acc_train: 0.7000 loss_val: 1.8136 acc_val: 0.5360 time: 0.0037s\n",
      "Epoch: 0036 loss_train: 1.6941 acc_train: 0.7643 loss_val: 1.8122 acc_val: 0.4880 time: 0.0037s\n",
      "Epoch: 0037 loss_train: 1.6913 acc_train: 0.7571 loss_val: 1.8007 acc_val: 0.5040 time: 0.0033s\n",
      "Epoch: 0038 loss_train: 1.6511 acc_train: 0.8000 loss_val: 1.7829 acc_val: 0.4980 time: 0.0037s\n",
      "Epoch: 0039 loss_train: 1.6496 acc_train: 0.7500 loss_val: 1.7832 acc_val: 0.5320 time: 0.0037s\n",
      "Epoch: 0040 loss_train: 1.6408 acc_train: 0.7429 loss_val: 1.7640 acc_val: 0.5320 time: 0.0031s\n",
      "Epoch: 0041 loss_train: 1.6273 acc_train: 0.7429 loss_val: 1.7766 acc_val: 0.5040 time: 0.0031s\n",
      "Epoch: 0042 loss_train: 1.5900 acc_train: 0.7929 loss_val: 1.7562 acc_val: 0.5240 time: 0.0032s\n",
      "Epoch: 0043 loss_train: 1.6072 acc_train: 0.7357 loss_val: 1.7386 acc_val: 0.5340 time: 0.0037s\n",
      "Epoch: 0044 loss_train: 1.5668 acc_train: 0.7786 loss_val: 1.7406 acc_val: 0.5120 time: 0.0037s\n",
      "Epoch: 0045 loss_train: 1.5500 acc_train: 0.7643 loss_val: 1.7324 acc_val: 0.5140 time: 0.0037s\n",
      "Epoch: 0046 loss_train: 1.5354 acc_train: 0.7429 loss_val: 1.6904 acc_val: 0.5520 time: 0.0036s\n",
      "Epoch: 0047 loss_train: 1.5275 acc_train: 0.7571 loss_val: 1.6912 acc_val: 0.5440 time: 0.0031s\n",
      "Epoch: 0048 loss_train: 1.5078 acc_train: 0.7857 loss_val: 1.6738 acc_val: 0.5700 time: 0.0037s\n",
      "Epoch: 0049 loss_train: 1.4936 acc_train: 0.8143 loss_val: 1.6537 acc_val: 0.5500 time: 0.0038s\n",
      "Epoch: 0050 loss_train: 1.4739 acc_train: 0.8214 loss_val: 1.6567 acc_val: 0.5820 time: 0.0034s\n",
      "Epoch: 0051 loss_train: 1.4610 acc_train: 0.8143 loss_val: 1.6497 acc_val: 0.5660 time: 0.0034s\n",
      "Epoch: 0052 loss_train: 1.4372 acc_train: 0.7857 loss_val: 1.6523 acc_val: 0.5580 time: 0.0033s\n",
      "Epoch: 0053 loss_train: 1.3878 acc_train: 0.8286 loss_val: 1.6236 acc_val: 0.5640 time: 0.0047s\n",
      "Epoch: 0054 loss_train: 1.3519 acc_train: 0.8643 loss_val: 1.6058 acc_val: 0.5900 time: 0.0035s\n",
      "Epoch: 0055 loss_train: 1.3923 acc_train: 0.8000 loss_val: 1.6084 acc_val: 0.5740 time: 0.0033s\n",
      "Epoch: 0056 loss_train: 1.3248 acc_train: 0.8143 loss_val: 1.5925 acc_val: 0.5820 time: 0.0039s\n",
      "Epoch: 0057 loss_train: 1.2971 acc_train: 0.8786 loss_val: 1.5541 acc_val: 0.6280 time: 0.0034s\n",
      "Epoch: 0058 loss_train: 1.3366 acc_train: 0.8143 loss_val: 1.5704 acc_val: 0.5760 time: 0.0034s\n",
      "Epoch: 0059 loss_train: 1.2769 acc_train: 0.8357 loss_val: 1.5657 acc_val: 0.5820 time: 0.0032s\n",
      "Epoch: 0060 loss_train: 1.2428 acc_train: 0.8429 loss_val: 1.5333 acc_val: 0.5900 time: 0.0033s\n",
      "Epoch: 0061 loss_train: 1.2476 acc_train: 0.8357 loss_val: 1.5608 acc_val: 0.5680 time: 0.0037s\n",
      "Epoch: 0062 loss_train: 1.2213 acc_train: 0.8286 loss_val: 1.5045 acc_val: 0.5940 time: 0.0037s\n",
      "Epoch: 0063 loss_train: 1.2329 acc_train: 0.8071 loss_val: 1.4842 acc_val: 0.6280 time: 0.0033s\n",
      "Epoch: 0064 loss_train: 1.1750 acc_train: 0.8786 loss_val: 1.4886 acc_val: 0.6140 time: 0.0030s\n",
      "Epoch: 0065 loss_train: 1.1418 acc_train: 0.8643 loss_val: 1.4670 acc_val: 0.5980 time: 0.0032s\n",
      "Epoch: 0066 loss_train: 1.1307 acc_train: 0.8857 loss_val: 1.4580 acc_val: 0.6220 time: 0.0033s\n",
      "Epoch: 0067 loss_train: 1.1126 acc_train: 0.8500 loss_val: 1.4455 acc_val: 0.6240 time: 0.0032s\n",
      "Epoch: 0068 loss_train: 1.1259 acc_train: 0.8571 loss_val: 1.4110 acc_val: 0.6540 time: 0.0032s\n",
      "Epoch: 0069 loss_train: 1.0761 acc_train: 0.8714 loss_val: 1.3947 acc_val: 0.6660 time: 0.0032s\n",
      "Epoch: 0070 loss_train: 1.0644 acc_train: 0.8857 loss_val: 1.3883 acc_val: 0.6720 time: 0.0038s\n",
      "Epoch: 0071 loss_train: 1.0412 acc_train: 0.8500 loss_val: 1.3645 acc_val: 0.6660 time: 0.0031s\n",
      "Epoch: 0072 loss_train: 1.0203 acc_train: 0.8714 loss_val: 1.3672 acc_val: 0.6520 time: 0.0032s\n",
      "Epoch: 0073 loss_train: 1.0200 acc_train: 0.8857 loss_val: 1.3627 acc_val: 0.6540 time: 0.0037s\n",
      "Epoch: 0074 loss_train: 0.9763 acc_train: 0.8714 loss_val: 1.3679 acc_val: 0.6820 time: 0.0030s\n",
      "Epoch: 0075 loss_train: 0.9807 acc_train: 0.9214 loss_val: 1.3238 acc_val: 0.6640 time: 0.0031s\n",
      "Epoch: 0076 loss_train: 0.9748 acc_train: 0.8929 loss_val: 1.3535 acc_val: 0.6340 time: 0.0034s\n",
      "Epoch: 0077 loss_train: 0.9197 acc_train: 0.9000 loss_val: 1.3302 acc_val: 0.6460 time: 0.0038s\n",
      "Epoch: 0078 loss_train: 0.9589 acc_train: 0.8214 loss_val: 1.3537 acc_val: 0.6460 time: 0.0035s\n",
      "Epoch: 0079 loss_train: 0.8593 acc_train: 0.8929 loss_val: 1.2897 acc_val: 0.6420 time: 0.0033s\n",
      "Epoch: 0080 loss_train: 0.8938 acc_train: 0.9071 loss_val: 1.3279 acc_val: 0.6400 time: 0.0038s\n",
      "Epoch: 0081 loss_train: 0.9024 acc_train: 0.8857 loss_val: 1.2688 acc_val: 0.6640 time: 0.0031s\n",
      "Epoch: 0082 loss_train: 0.8825 acc_train: 0.8786 loss_val: 1.2799 acc_val: 0.6340 time: 0.0031s\n",
      "Epoch: 0083 loss_train: 0.8856 acc_train: 0.8714 loss_val: 1.2297 acc_val: 0.6980 time: 0.0038s\n",
      "Epoch: 0084 loss_train: 0.8750 acc_train: 0.8714 loss_val: 1.2434 acc_val: 0.6680 time: 0.0034s\n",
      "Epoch: 0085 loss_train: 0.8516 acc_train: 0.9000 loss_val: 1.2459 acc_val: 0.6560 time: 0.0037s\n",
      "Epoch: 0086 loss_train: 0.8405 acc_train: 0.9000 loss_val: 1.2368 acc_val: 0.6860 time: 0.0033s\n",
      "Epoch: 0087 loss_train: 0.7932 acc_train: 0.9429 loss_val: 1.2126 acc_val: 0.6760 time: 0.0037s\n",
      "Epoch: 0088 loss_train: 0.8059 acc_train: 0.9000 loss_val: 1.2194 acc_val: 0.6860 time: 0.0038s\n",
      "Epoch: 0089 loss_train: 0.8276 acc_train: 0.8857 loss_val: 1.2141 acc_val: 0.7020 time: 0.0032s\n",
      "Epoch: 0090 loss_train: 0.7718 acc_train: 0.9500 loss_val: 1.2124 acc_val: 0.6760 time: 0.0034s\n",
      "Epoch: 0091 loss_train: 0.7771 acc_train: 0.9000 loss_val: 1.1907 acc_val: 0.6860 time: 0.0032s\n",
      "Epoch: 0092 loss_train: 0.7520 acc_train: 0.8857 loss_val: 1.1775 acc_val: 0.6880 time: 0.0031s\n",
      "Epoch: 0093 loss_train: 0.7404 acc_train: 0.9143 loss_val: 1.1556 acc_val: 0.6940 time: 0.0033s\n",
      "Epoch: 0094 loss_train: 0.7596 acc_train: 0.8500 loss_val: 1.1613 acc_val: 0.6860 time: 0.0031s\n",
      "Epoch: 0095 loss_train: 0.7443 acc_train: 0.9214 loss_val: 1.1572 acc_val: 0.7160 time: 0.0031s\n",
      "Epoch: 0096 loss_train: 0.7008 acc_train: 0.9214 loss_val: 1.1621 acc_val: 0.6820 time: 0.0033s\n",
      "Epoch: 0097 loss_train: 0.7350 acc_train: 0.9286 loss_val: 1.1409 acc_val: 0.6980 time: 0.0034s\n",
      "Epoch: 0098 loss_train: 0.7415 acc_train: 0.9000 loss_val: 1.1315 acc_val: 0.7180 time: 0.0038s\n",
      "Epoch: 0099 loss_train: 0.6441 acc_train: 0.9571 loss_val: 1.1103 acc_val: 0.7200 time: 0.0038s\n",
      "Epoch: 0100 loss_train: 0.6874 acc_train: 0.9214 loss_val: 1.1002 acc_val: 0.7040 time: 0.0038s\n",
      "Epoch: 0101 loss_train: 0.7520 acc_train: 0.8929 loss_val: 1.1687 acc_val: 0.6680 time: 0.0034s\n",
      "Epoch: 0102 loss_train: 0.7042 acc_train: 0.8857 loss_val: 1.1618 acc_val: 0.6800 time: 0.0033s\n",
      "Epoch: 0103 loss_train: 0.6486 acc_train: 0.9143 loss_val: 1.1228 acc_val: 0.6880 time: 0.0031s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0104 loss_train: 0.6244 acc_train: 0.9286 loss_val: 1.1008 acc_val: 0.7260 time: 0.0033s\n",
      "Epoch: 0105 loss_train: 0.6456 acc_train: 0.9500 loss_val: 1.1147 acc_val: 0.6900 time: 0.0032s\n",
      "Epoch: 0106 loss_train: 0.6476 acc_train: 0.9286 loss_val: 1.0685 acc_val: 0.7200 time: 0.0038s\n",
      "Epoch: 0107 loss_train: 0.6485 acc_train: 0.9071 loss_val: 1.0822 acc_val: 0.7280 time: 0.0032s\n",
      "Epoch: 0108 loss_train: 0.6133 acc_train: 0.9500 loss_val: 1.0931 acc_val: 0.7040 time: 0.0035s\n",
      "Epoch: 0109 loss_train: 0.6032 acc_train: 0.9286 loss_val: 1.0536 acc_val: 0.7140 time: 0.0031s\n",
      "Epoch: 0110 loss_train: 0.6125 acc_train: 0.9357 loss_val: 1.1051 acc_val: 0.6720 time: 0.0038s\n",
      "Epoch: 0111 loss_train: 0.5982 acc_train: 0.9286 loss_val: 1.0490 acc_val: 0.7120 time: 0.0038s\n",
      "Epoch: 0112 loss_train: 0.6350 acc_train: 0.9500 loss_val: 1.0980 acc_val: 0.6760 time: 0.0038s\n",
      "Epoch: 0113 loss_train: 0.6136 acc_train: 0.9286 loss_val: 1.0268 acc_val: 0.7560 time: 0.0038s\n",
      "Epoch: 0114 loss_train: 0.5899 acc_train: 0.9500 loss_val: 1.0745 acc_val: 0.6940 time: 0.0031s\n",
      "Epoch: 0115 loss_train: 0.5923 acc_train: 0.9357 loss_val: 1.0550 acc_val: 0.7040 time: 0.0034s\n",
      "Epoch: 0116 loss_train: 0.6070 acc_train: 0.9286 loss_val: 1.0582 acc_val: 0.7160 time: 0.0031s\n",
      "Epoch: 0117 loss_train: 0.5538 acc_train: 0.9500 loss_val: 1.0090 acc_val: 0.7400 time: 0.0033s\n",
      "Epoch: 0118 loss_train: 0.5570 acc_train: 0.9500 loss_val: 1.0603 acc_val: 0.6860 time: 0.0031s\n",
      "Epoch: 0119 loss_train: 0.5194 acc_train: 0.9643 loss_val: 1.0409 acc_val: 0.7100 time: 0.0032s\n",
      "Epoch: 0120 loss_train: 0.5391 acc_train: 0.9571 loss_val: 1.0284 acc_val: 0.6960 time: 0.0032s\n",
      "Epoch: 0121 loss_train: 0.5958 acc_train: 0.9429 loss_val: 1.0623 acc_val: 0.7160 time: 0.0038s\n",
      "Epoch: 0122 loss_train: 0.5098 acc_train: 0.9500 loss_val: 1.0491 acc_val: 0.6880 time: 0.0033s\n",
      "Epoch: 0123 loss_train: 0.5519 acc_train: 0.9500 loss_val: 1.0587 acc_val: 0.6760 time: 0.0033s\n",
      "Epoch: 0124 loss_train: 0.5163 acc_train: 0.9500 loss_val: 0.9975 acc_val: 0.7300 time: 0.0032s\n",
      "Epoch: 0125 loss_train: 0.5269 acc_train: 0.9357 loss_val: 1.0106 acc_val: 0.7060 time: 0.0033s\n",
      "Epoch: 0126 loss_train: 0.5308 acc_train: 0.9429 loss_val: 1.0273 acc_val: 0.6940 time: 0.0031s\n",
      "Epoch: 0127 loss_train: 0.5016 acc_train: 0.9571 loss_val: 1.0015 acc_val: 0.7320 time: 0.0034s\n",
      "Epoch: 0128 loss_train: 0.5022 acc_train: 0.9643 loss_val: 1.0439 acc_val: 0.6960 time: 0.0031s\n",
      "Epoch: 0129 loss_train: 0.4774 acc_train: 0.9643 loss_val: 1.0304 acc_val: 0.7240 time: 0.0038s\n",
      "Epoch: 0130 loss_train: 0.5024 acc_train: 0.9500 loss_val: 1.0156 acc_val: 0.6980 time: 0.0032s\n",
      "Epoch: 0131 loss_train: 0.4972 acc_train: 0.9571 loss_val: 1.0135 acc_val: 0.7260 time: 0.0032s\n",
      "Epoch: 0132 loss_train: 0.5251 acc_train: 0.9214 loss_val: 0.9802 acc_val: 0.7500 time: 0.0034s\n",
      "Epoch: 0133 loss_train: 0.4990 acc_train: 0.9286 loss_val: 1.0102 acc_val: 0.7080 time: 0.0033s\n",
      "Epoch: 0134 loss_train: 0.5145 acc_train: 0.9357 loss_val: 0.9971 acc_val: 0.7220 time: 0.0032s\n",
      "Epoch: 0135 loss_train: 0.4854 acc_train: 0.9286 loss_val: 1.0109 acc_val: 0.6960 time: 0.0034s\n",
      "Epoch: 0136 loss_train: 0.4866 acc_train: 0.9429 loss_val: 0.9849 acc_val: 0.7080 time: 0.0031s\n",
      "Epoch: 0137 loss_train: 0.4754 acc_train: 0.9643 loss_val: 0.9607 acc_val: 0.7060 time: 0.0030s\n",
      "Epoch: 0138 loss_train: 0.4623 acc_train: 0.9571 loss_val: 0.9743 acc_val: 0.7200 time: 0.0030s\n",
      "Epoch: 0139 loss_train: 0.4899 acc_train: 0.9286 loss_val: 0.9703 acc_val: 0.7280 time: 0.0037s\n",
      "Epoch: 0140 loss_train: 0.4574 acc_train: 0.9500 loss_val: 0.9544 acc_val: 0.7200 time: 0.0035s\n",
      "Epoch: 0141 loss_train: 0.4733 acc_train: 0.9214 loss_val: 0.9875 acc_val: 0.6940 time: 0.0034s\n",
      "Epoch: 0142 loss_train: 0.4701 acc_train: 0.9286 loss_val: 0.9791 acc_val: 0.7000 time: 0.0035s\n",
      "Epoch: 0143 loss_train: 0.4776 acc_train: 0.9571 loss_val: 0.9520 acc_val: 0.7380 time: 0.0033s\n",
      "Epoch: 0144 loss_train: 0.4721 acc_train: 0.9500 loss_val: 0.9991 acc_val: 0.6980 time: 0.0039s\n",
      "Epoch: 0145 loss_train: 0.4542 acc_train: 0.9357 loss_val: 0.9994 acc_val: 0.7240 time: 0.0030s\n",
      "Epoch: 0146 loss_train: 0.4333 acc_train: 0.9786 loss_val: 0.9317 acc_val: 0.7460 time: 0.0032s\n",
      "Epoch: 0147 loss_train: 0.4800 acc_train: 0.9643 loss_val: 1.0042 acc_val: 0.7020 time: 0.0031s\n",
      "Epoch: 0148 loss_train: 0.4586 acc_train: 0.9643 loss_val: 0.9710 acc_val: 0.7260 time: 0.0034s\n",
      "Epoch: 0149 loss_train: 0.4162 acc_train: 0.9714 loss_val: 0.9404 acc_val: 0.7440 time: 0.0032s\n",
      "Epoch: 0150 loss_train: 0.4242 acc_train: 0.9571 loss_val: 0.9706 acc_val: 0.7080 time: 0.0034s\n",
      "Epoch: 0151 loss_train: 0.4519 acc_train: 0.9357 loss_val: 0.9391 acc_val: 0.7380 time: 0.0033s\n",
      "Epoch: 0152 loss_train: 0.4227 acc_train: 0.9714 loss_val: 0.9488 acc_val: 0.7220 time: 0.0037s\n",
      "Epoch: 0153 loss_train: 0.4552 acc_train: 0.9429 loss_val: 0.9690 acc_val: 0.6980 time: 0.0033s\n",
      "Epoch: 0154 loss_train: 0.4513 acc_train: 0.9429 loss_val: 0.9555 acc_val: 0.7100 time: 0.0032s\n",
      "Epoch: 0155 loss_train: 0.4169 acc_train: 0.9500 loss_val: 0.9645 acc_val: 0.7060 time: 0.0037s\n",
      "Epoch: 0156 loss_train: 0.3994 acc_train: 0.9500 loss_val: 0.9280 acc_val: 0.7380 time: 0.0032s\n",
      "Epoch: 0157 loss_train: 0.4477 acc_train: 0.9357 loss_val: 0.9736 acc_val: 0.7200 time: 0.0036s\n",
      "Epoch: 0158 loss_train: 0.4348 acc_train: 0.9500 loss_val: 0.9417 acc_val: 0.7240 time: 0.0032s\n",
      "Epoch: 0159 loss_train: 0.4108 acc_train: 0.9571 loss_val: 0.9323 acc_val: 0.7380 time: 0.0037s\n",
      "Epoch: 0160 loss_train: 0.4213 acc_train: 0.9429 loss_val: 0.9676 acc_val: 0.7120 time: 0.0037s\n",
      "Epoch: 0161 loss_train: 0.4094 acc_train: 0.9571 loss_val: 0.9401 acc_val: 0.7320 time: 0.0032s\n",
      "Epoch: 0162 loss_train: 0.4061 acc_train: 0.9643 loss_val: 0.9241 acc_val: 0.7280 time: 0.0031s\n",
      "Epoch: 0163 loss_train: 0.4515 acc_train: 0.9286 loss_val: 0.9488 acc_val: 0.7180 time: 0.0034s\n",
      "Epoch: 0164 loss_train: 0.4276 acc_train: 0.9500 loss_val: 0.9731 acc_val: 0.7080 time: 0.0032s\n",
      "Epoch: 0165 loss_train: 0.4442 acc_train: 0.9500 loss_val: 0.9614 acc_val: 0.7360 time: 0.0038s\n",
      "Epoch: 0166 loss_train: 0.4149 acc_train: 0.9571 loss_val: 0.8821 acc_val: 0.7600 time: 0.0035s\n",
      "Epoch: 0167 loss_train: 0.4240 acc_train: 0.9500 loss_val: 0.9351 acc_val: 0.7140 time: 0.0038s\n",
      "Epoch: 0168 loss_train: 0.4113 acc_train: 0.9571 loss_val: 0.9318 acc_val: 0.7180 time: 0.0037s\n",
      "Epoch: 0169 loss_train: 0.3687 acc_train: 0.9571 loss_val: 0.8828 acc_val: 0.7580 time: 0.0037s\n",
      "Epoch: 0170 loss_train: 0.3829 acc_train: 0.9786 loss_val: 0.9059 acc_val: 0.7420 time: 0.0037s\n",
      "Epoch: 0171 loss_train: 0.3947 acc_train: 0.9857 loss_val: 0.9358 acc_val: 0.7240 time: 0.0038s\n",
      "Epoch: 0172 loss_train: 0.3917 acc_train: 0.9500 loss_val: 0.8671 acc_val: 0.7460 time: 0.0036s\n",
      "Epoch: 0173 loss_train: 0.3903 acc_train: 0.9571 loss_val: 0.9281 acc_val: 0.7280 time: 0.0042s\n",
      "Epoch: 0174 loss_train: 0.3630 acc_train: 0.9786 loss_val: 0.9667 acc_val: 0.7060 time: 0.0043s\n",
      "Epoch: 0175 loss_train: 0.3919 acc_train: 0.9643 loss_val: 0.8919 acc_val: 0.7340 time: 0.0035s\n",
      "Epoch: 0176 loss_train: 0.4114 acc_train: 0.9571 loss_val: 0.9219 acc_val: 0.7360 time: 0.0037s\n",
      "Epoch: 0177 loss_train: 0.3574 acc_train: 0.9714 loss_val: 0.8836 acc_val: 0.7300 time: 0.0028s\n",
      "Epoch: 0178 loss_train: 0.3648 acc_train: 0.9714 loss_val: 0.9527 acc_val: 0.7340 time: 0.0030s\n",
      "Epoch: 0179 loss_train: 0.3499 acc_train: 0.9714 loss_val: 0.8829 acc_val: 0.7580 time: 0.0028s\n",
      "Epoch: 0180 loss_train: 0.3859 acc_train: 0.9643 loss_val: 0.9038 acc_val: 0.7460 time: 0.0032s\n",
      "Epoch: 0181 loss_train: 0.3590 acc_train: 0.9643 loss_val: 0.9153 acc_val: 0.7220 time: 0.0032s\n",
      "Epoch: 0182 loss_train: 0.4022 acc_train: 0.9214 loss_val: 0.8633 acc_val: 0.7380 time: 0.0032s\n",
      "Epoch: 0183 loss_train: 0.3643 acc_train: 0.9643 loss_val: 0.8935 acc_val: 0.7260 time: 0.0032s\n",
      "Epoch: 0184 loss_train: 0.3391 acc_train: 0.9714 loss_val: 0.8885 acc_val: 0.7340 time: 0.0032s\n",
      "Epoch: 0185 loss_train: 0.4020 acc_train: 0.9714 loss_val: 0.8867 acc_val: 0.7300 time: 0.0032s\n",
      "Epoch: 0186 loss_train: 0.4258 acc_train: 0.9429 loss_val: 0.9165 acc_val: 0.7360 time: 0.0032s\n",
      "Epoch: 0187 loss_train: 0.3254 acc_train: 0.9857 loss_val: 0.9122 acc_val: 0.7180 time: 0.0034s\n",
      "Epoch: 0188 loss_train: 0.3634 acc_train: 0.9643 loss_val: 0.9055 acc_val: 0.7560 time: 0.0034s\n",
      "Epoch: 0189 loss_train: 0.3587 acc_train: 0.9786 loss_val: 0.9597 acc_val: 0.7040 time: 0.0035s\n",
      "Epoch: 0190 loss_train: 0.3716 acc_train: 0.9643 loss_val: 0.9366 acc_val: 0.7080 time: 0.0035s\n",
      "Epoch: 0191 loss_train: 0.3836 acc_train: 0.9643 loss_val: 0.8946 acc_val: 0.7300 time: 0.0035s\n",
      "Epoch: 0192 loss_train: 0.3606 acc_train: 0.9500 loss_val: 0.8959 acc_val: 0.7380 time: 0.0035s\n",
      "Epoch: 0193 loss_train: 0.3708 acc_train: 0.9643 loss_val: 0.9038 acc_val: 0.7300 time: 0.0031s\n",
      "Epoch: 0194 loss_train: 0.3364 acc_train: 0.9500 loss_val: 0.8937 acc_val: 0.7400 time: 0.0035s\n",
      "Epoch: 0195 loss_train: 0.3683 acc_train: 0.9714 loss_val: 0.8865 acc_val: 0.7340 time: 0.0035s\n",
      "Epoch: 0196 loss_train: 0.3766 acc_train: 0.9357 loss_val: 0.9396 acc_val: 0.7260 time: 0.0031s\n",
      "Epoch: 0197 loss_train: 0.3176 acc_train: 0.9786 loss_val: 0.8867 acc_val: 0.7280 time: 0.0028s\n",
      "Epoch: 0198 loss_train: 0.3507 acc_train: 0.9571 loss_val: 0.8930 acc_val: 0.7380 time: 0.0035s\n",
      "Epoch: 0199 loss_train: 0.3704 acc_train: 0.9429 loss_val: 0.9070 acc_val: 0.7100 time: 0.0035s\n",
      "Epoch: 0200 loss_train: 0.3282 acc_train: 0.9714 loss_val: 0.8800 acc_val: 0.7240 time: 0.0035s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.0694s\n",
      "Test set results: loss= 0.7341 accuracy= 0.8060\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# torch.save(model, './cora_gcn.pth')\n",
    "# torch.save(model.state_dict(), 'cora_gcn.pkl')\n",
    "\n",
    "# Testing\n",
    "ori_output = test(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_perturb(input_adj, idx, perturb):\n",
    "    # (1-x)A + x(1-A)\n",
    "#     input_adj = input_adj.toarray()\n",
    "\n",
    "    x = np.zeros((input_adj.shape[0], input_adj.shape[1]))\n",
    "    x[idx] = perturb  \n",
    "    x[:,idx] = perturb\n",
    "#     print ('x', x[idx])\n",
    "\n",
    "    \n",
    "#     x += np.transpose(x) #change the idx'th row and column\n",
    "    x1 = np.ones((input_adj.shape[0], input_adj.shape[1])) - x\n",
    "#     print ('x1', x1[idx])\n",
    "    adj2 = np.ones((input_adj.shape[0], input_adj.shape[1])) - input_adj\n",
    "#     print ('adj2', adj2[idx])\n",
    "\n",
    "    for i in range(input_adj.shape[0]):      \n",
    "        adj2[i][i] = 0\n",
    "\n",
    "    perturbed_adj = np.multiply(x1, input_adj) + np.multiply(x, adj2)\n",
    "    return perturbed_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(perturb):\n",
    "    res = []\n",
    "    # perturb = np.where(perturb>0.5, 1, 0)\n",
    "    print ('perturb', perturb)\n",
    "    new_pred = []\n",
    "    for i in range(num_classes):\n",
    "        new_pred.append(0)\n",
    "    for k in idx_test:\n",
    "#     for k in range(1):\n",
    "#         print ('test node', k)\n",
    "        innormal_x_p = add_perturb(tmp_adj, k, perturb)\n",
    "#         print ('the perturbed conn', sum(innormal_x_p[k]))\n",
    "#         innormal_x_p = np.where(innormal_x_p<0.5, 0, 1)\n",
    "\n",
    "#         diff = innormal_x_p[k] - tmp_adj[k]\n",
    "#         diff_idx = np.where(diff != 0 )\n",
    "        \n",
    "#         print ('diff_idx', diff_idx)\n",
    "    #     one_idx = np.where(innormal_x_p[k]==1)[0]\n",
    "    #     zero_idx = np.where(innormal_x_p[k]!=1)[0]\n",
    "    #     total_idx = one_idx.shape[0] + zero_idx.shape[0]\n",
    "    #     print ('total_idx', total_idx)\n",
    "    #     print ('one_idx', one_idx)\n",
    "    #     print ('corresponding perturb', perturb[one_idx])\n",
    "    #     print (innormal_x_p[k][one_idx])\n",
    "        x_p, degree_p = normalize(innormal_x_p + np.eye(tmp_adj.shape[0]))\n",
    "        x_p = torch.from_numpy(x_p.astype(np.float32))\n",
    "        x_p = x_p.cuda()\n",
    "        output = model(features, x_p)\n",
    "        new_pred[int(torch.argmax(output[k]))] += 1\n",
    "        if int(torch.argmax(output[k])) == int(torch.argmax(ori_output[k])):\n",
    "            res.append(0)\n",
    "            print ('node {} attack failed'.format(k))\n",
    "        else:\n",
    "            res.append(1)\n",
    "            print ('node {} attack succeed'.format(k))\n",
    "    fooling_rate = float(sum(res)/len(res))\n",
    "    print ('the current fooling rate is', fooling_rate)\n",
    "    return fooling_rate, new_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(pred):\n",
    "    h = 0\n",
    "    all_pred = sum(pred)\n",
    "    for i in range(num_classes):\n",
    "        Pi = pred[i]/all_pred\n",
    "        if Pi != 0:\n",
    "            h -=  Pi* math.log(Pi)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the entropy is 1.863093342917643\n"
     ]
    }
   ],
   "source": [
    "new_pred = []\n",
    "for i in range(num_classes):\n",
    "    new_pred.append(0)\n",
    "for k in idx_test:\n",
    "    new_pred[int(torch.argmax(ori_output[k]))] += 1\n",
    "entropy = calculate_entropy(new_pred)\n",
    "print ('the entropy is', entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the perturbation is [  12   29   86 2400 2662]\n",
      "perturb [0 0 0 ... 0 0 0]\n",
      "node 1708 attack succeed\n",
      "node 1709 attack succeed\n",
      "node 1710 attack succeed\n",
      "node 1711 attack succeed\n",
      "node 1712 attack succeed\n",
      "node 1713 attack succeed\n",
      "node 1714 attack succeed\n",
      "node 1715 attack succeed\n",
      "node 1716 attack succeed\n",
      "node 1717 attack succeed\n",
      "node 1718 attack failed\n",
      "node 1719 attack failed\n",
      "node 1720 attack failed\n",
      "node 1721 attack failed\n",
      "node 1722 attack succeed\n",
      "node 1723 attack succeed\n",
      "node 1724 attack succeed\n",
      "node 1725 attack failed\n",
      "node 1726 attack succeed\n",
      "node 1727 attack succeed\n",
      "node 1728 attack succeed\n",
      "node 1729 attack succeed\n",
      "node 1730 attack succeed\n",
      "node 1731 attack succeed\n",
      "node 1732 attack succeed\n",
      "node 1733 attack succeed\n",
      "node 1734 attack succeed\n",
      "node 1735 attack failed\n",
      "node 1736 attack succeed\n",
      "node 1737 attack succeed\n",
      "node 1738 attack failed\n",
      "node 1739 attack failed\n",
      "node 1740 attack failed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5fe3aa0cb9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'the perturbation is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'the prediction result is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-09232120dcf8>\u001b[0m in \u001b[0;36mevaluate_attack\u001b[0;34m(perturb)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     for k in range(1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         print ('test node', k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0minnormal_x_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_perturb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_adj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print ('the perturbed conn', sum(innormal_x_p[k]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         innormal_x_p = np.where(innormal_x_p<0.5, 0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dc7cd1ac3ec3>\u001b[0m in \u001b[0;36madd_perturb\u001b[0;34m(input_adj, idx, perturb)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print ('x1', x1[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0madj2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minput_adj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#     print ('adj2', adj2[idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluate the universal attack\n",
    "if args.evaluate_mode == \"universal\":\n",
    "    fool_res = []\n",
    "    p_times = []\n",
    "    all_entropy = []\n",
    "    for i in range(10):\n",
    "        perturb = np.array([float(line.rstrip('\\n')) for line in open('./perturbation_results/{1}_xi{2}_epoch100/perturbation_{1}_{0}.txt'.format(i, args.dataset, args.radius))])\n",
    "        perturb = np.where(perturb>0.5, 1, 0)\n",
    "        pt = np.where(perturb>0)[0]\n",
    "        if len(list(pt)) == 0:\n",
    "            fool_res.append(0)\n",
    "            p_times.append(0)\n",
    "            continue\n",
    "        print ('the perturbation is', pt)\n",
    "        res, new_pred = evaluate_attack(perturb)\n",
    "        print ('the prediction result is', new_pred)\n",
    "        entropy = calculate_entropy(new_pred)\n",
    "        fool_res.append(res)      \n",
    "        p_times.append(len(list(pt)))\n",
    "        print ('the perturbation times is', p_times)\n",
    "        print ('the fooling rates are', fool_res)\n",
    "        print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "        print ('the entropy is', entropy)\n",
    "        all_entropy.append(entropy)\n",
    "    print ('all the entropy values are', all_entropy)\n",
    "    print ('the average entropy is', sum(all_entropy)/float(len(all_entropy)))\n",
    "        \n",
    "elif args.evaluate_mode == \"unlimitted_random\":\n",
    "    fool_res = []\n",
    "    p_times = []\n",
    "    for i in range(10):\n",
    "        perturb = np.random.rand(tmp_adj.shape[0])\n",
    "        perturb = np.where(perturb>0.5, 1, 0)\n",
    "        pt = np.where(perturb>0)[0]\n",
    "        print ('the perturbation is', len(list(pt)))\n",
    "        res = evaluate_attack(perturb)\n",
    "        fool_res.append(res)\n",
    "        \n",
    "        p_times.append(len(list(pt)))\n",
    "        print ('the perturbation times is', p_times)\n",
    "        print ('the fooling rates are', fool_res)\n",
    "        print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "elif args.evaluate_mode == \"limitted_random\":\n",
    "    \n",
    "    perturb_times = 8\n",
    "    fool_res = []\n",
    "    p_times = []\n",
    "    for i in range(10):\n",
    "#         perturb = np.array([float(line.rstrip('\\n')) for line in open(\"perturbation.txt\")])\n",
    "#         perturb = np.where(perturb>0.5, 1, 0)\n",
    "#         perturb_times = sum(perturb)\n",
    "        perturb = np.zeros(adj.shape[1])\n",
    "        #the perturbation times of our universal perturbation\n",
    "        attack_index = list(np.random.choice(range(adj.shape[1]), perturb_times, replace = False))\n",
    "        perturb[attack_index] = 1\n",
    "        pt = np.where(perturb>0)[0]\n",
    "#         print ('the perturbation is', pt)\n",
    "        res = evaluate_attack(perturb)\n",
    "        fool_res.append(res)\n",
    "\n",
    "        p_times.append(len(list(pt)))\n",
    "        print ('the perturbation times is', p_times)\n",
    "        print ('the fooling rates are', fool_res)\n",
    "        print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "\n",
    "    print ('the average fooling rate with {} perturbation times is'.format(perturb_times), sum(fool_res)/float(len(fool_res)))\n",
    "    \n",
    "elif args.evaluate_mode == \"PerturbUsingTargetClass\":\n",
    "    #the perturbation times of our universal perturbation\n",
    "    perturb_time = 8 #set this equal to the ceil of the number of anchor nodes computed by universal attack\n",
    "    fool_res = []\n",
    "#     p_times = []\n",
    "    for k in range(num_classes):\n",
    "#     for k in range(4,6):\n",
    "        each_fool_res = []\n",
    "        idx = np.where(labels.cpu().numpy()==k)[0]\n",
    "#         for i in range(1):\n",
    "        for i in range(10):\n",
    "            attack_index = list(np.random.choice(idx, perturb_time, replace = False))\n",
    "            perturb = np.zeros(adj.shape[1])\n",
    "            perturb[attack_index] = 1\n",
    "            print ('perturbating by connecting to nodes of class', k)\n",
    "            res = evaluate_attack(perturb)\n",
    "            each_fool_res.append(res)\n",
    "            print ('the fooling rates of current class are', each_fool_res)\n",
    "            avg_asr = sum(each_fool_res)/float(len(each_fool_res))\n",
    "            print ('the average fooling rates over 10 times of test is', avg_asr)\n",
    "        fool_res.append(avg_asr)\n",
    "        print ('fool_res', fool_res)\n",
    "    print ('the avg asr by connecting to each class of nodes is', fool_res)\n",
    "        \n",
    "elif args.evaluate_mode == \"universal_delete\":   \n",
    "    \n",
    "    all_fool = []\n",
    "    for i in range(8, 9):\n",
    "        fool_res = []\n",
    "        for j in range(8):\n",
    "            perturb = np.array([float(line.rstrip('\\n')) for line in open('./perturbation_results/{1}_xi{2}_epoch100/perturbation_{1}_4.txt'.format(i, args.dataset, args.radius))])\n",
    "            perturb = np.where(perturb>0.5, 1, 0)\n",
    "            pt = np.where(perturb>0)[0]\n",
    "            a = list(np.random.choice(range(0, pt.shape[0]), i, replace = False))\n",
    "            perturb[pt[a]] = 0\n",
    "            pt = np.where(perturb>0)[0]\n",
    "            print ('the perturbation is', pt)\n",
    "            res, new_pred = evaluate_attack(perturb)\n",
    "            fool_res.append(res)      \n",
    "            print ('the fooling rates are', fool_res)\n",
    "            print ('the average fooling rates over 10 times of test is', sum(fool_res)/float(len(fool_res)))\n",
    "        all_fool.append(sum(fool_res)/float(len(fool_res)))\n",
    "          \n",
    "    print ('all the fooling rate is', all_fool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
